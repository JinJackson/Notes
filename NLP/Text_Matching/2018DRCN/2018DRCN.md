# DRCN

题目：Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information

年份：2018

## 动机：

传统的注意力机制无法保存多层原始的特征，根据DenseNet的启发，作者将循环网络的隐层的输出与最后一层连接。

另外加入注意力机制，代替原来的卷积。由于最后的特征维度过大，加入AE降维。



## 模型结构：

![img](D:\Dev\typoraspace\notes\NLP\Text_Matching\2018DRCN\imgs\model_structure.png)

## 模型过程：

+ ### Word Representation Layer 
![image-20201107104923236](D:\Dev\typoraspace\notes\NLP\Text_Matching\2018DRCN\imgs\representation_layer.png)

+ ### Densely connected Recurrent Networks 

  作者收DenseNet启发，使用了密集连接和RNN结合的方法来实现对对句子的处理。

  + 传统的RNN

  ![image-20201107105149261](D:\Dev\typoraspace\notes\NLP\Text_Matching\2018DRCN\imgs\rnn1.png)

  + 残差连接的RNN

  ![image-20201107105211913](D:\Dev\typoraspace\notes\NLP\Text_Matching\2018DRCN\imgs\rnn2.png)

  + DenseNet连接的RNN

  ![image-20201107105234416](D:\Dev\typoraspace\notes\NLP\Text_Matching\2018DRCN\imgs\denseNetRNN.png)

+ ### Densely-connected Co-attentive networks

  因为句子匹配考虑的两个句子之间关系，因此需要建模两个句子之间的交互，目前来说，注意力机制是一种非常好的方法，因此作者在这样也使用了注意力机制，在RNN的每一层使用soft-alignment,最后将权重通过拼接加入到全连接，避免信息的丢失

  ![image-20201107111239839](D:\Dev\typoraspace\notes\NLP\Text_Matching\2018DRCN\imgs\attn1.png)

  ![image-20201107111253507](D:\Dev\typoraspace\notes\NLP\Text_Matching\2018DRCN\imgs\attn2.png)

+ ### Bottleneck component 

  正如前边提到的，这种dense连接方式直接导致的一个问题就是随着模型的加深，参数量会变的越来越多，这样最后全连接层的压力就会特别大。因此作者在这里使用了一个AutoEncoder来解决这个问题。AutoEncoder可以帮助压缩得到的巨大向量表示，同时可以保持原始的信息。

+ ### Pooling Layer

  Max Pooling得到维度相同的P、Q表示。

+ ### Interaction Layer

  ![image-20201107111633529](D:\Dev\typoraspace\notes\NLP\Text_Matching\2018DRCN\imgs\interaction.png)

  全连接和soft Max得到P、Q之间的关系

## 实验

![image-20201107111742370](D:\Dev\typoraspace\notes\NLP\Text_Matching\2018DRCN\imgs\exp1.png)

+ ### 匹配任务

  ![image-20201107111904082](D:\Dev\typoraspace\notes\NLP\Text_Matching\2018DRCN\imgs\exp3.png)

+ ### 消解实验

![image-20201107111815449](D:\Dev\typoraspace\notes\NLP\Text_Matching\2018DRCN\imgs\exp2.png)



## $x_l = H_l(x_{l-1})+ x_{l_1}$

## $x_l = H_l([x_0; x_1;...;x_{l-1}])$